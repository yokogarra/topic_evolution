{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90aacfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\55347\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\55347\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, re, argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import PartOfSpeech, KeyBERTInspired, MaximalMarginalRelevance\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "try:\n",
    "    from fugashi import Tagger\n",
    "    tagger = Tagger()\n",
    "except Exception:\n",
    "    tagger = None\n",
    "\n",
    "# ------------------------\n",
    "# Tokenizer for Japanese\n",
    "# ------------------------\n",
    "JA_STOPWORDS = set(\"の は に を が で と も から まで など そして しかし また その この あの する なる いる ある\".split())\n",
    "PUNCT = re.compile(r\"[。、・「」『』（）()［］\\[\\]【】〈〉《》…—\\-.,!?]\")\n",
    "\n",
    "def ja_tokenize(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = PUNCT.sub(\" \", text)\n",
    "    if tagger is None:\n",
    "        return [t for t in re.split(r\"\\s+\", text) if t]\n",
    "    toks = []\n",
    "    for w in tagger(text):\n",
    "        pos = getattr(w.feature, \"pos1\", \"\")\n",
    "        if pos in {\"助詞\",\"助動詞\",\"記号\",\"接続詞\",\"連体詞\",\"フィラー\",\"感動詞\"}:\n",
    "            continue\n",
    "        lemma = getattr(w.feature, \"lemma\", None) or w.surface\n",
    "        lemma = lemma.strip()\n",
    "        if not lemma or lemma in JA_STOPWORDS:\n",
    "            continue\n",
    "        toks.append(lemma)\n",
    "    return toks\n",
    "\n",
    "# ------------------------\n",
    "# Build BERTopic model\n",
    "# ------------------------\n",
    "def build_model():\n",
    "    vectorizer = CountVectorizer(\n",
    "        tokenizer=ja_tokenize,\n",
    "        token_pattern=None,\n",
    "        ngram_range=(1,2),\n",
    "        min_df=3, max_df=0.5\n",
    "    )\n",
    "\n",
    "    rep = [KeyBERTInspired(top_n_words=20),\n",
    "           MaximalMarginalRelevance(diversity=0.3)]\n",
    "\n",
    "    emb_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    ctfidf = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "    model = BERTopic(\n",
    "        embedding_model=emb_model,\n",
    "        ctfidf_model=ctfidf,\n",
    "        vectorizer_model=vectorizer,\n",
    "        representation_model=rep,\n",
    "        language=\"japanese\",\n",
    "        verbose=True,\n",
    "        calculate_probabilities=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ------------------------\n",
    "# Evolution alignment\n",
    "# ------------------------\n",
    "def align_topics(year_topics, threshold=0.6):\n",
    "    edges = []\n",
    "    years = sorted(year_topics.keys())\n",
    "    for i in range(len(years)-1):\n",
    "        y1, y2 = years[i], years[i+1]\n",
    "        nodes1, nodes2 = year_topics[y1], year_topics[y2]\n",
    "        if not nodes1 or not nodes2:\n",
    "            continue\n",
    "        emb1 = np.vstack([n['embedding'] for n in nodes1])\n",
    "        emb2 = np.vstack([n['embedding'] for n in nodes2])\n",
    "        sims = cosine_similarity(emb1, emb2)\n",
    "        for i1, n1 in enumerate(nodes1):\n",
    "            for i2, n2 in enumerate(nodes2):\n",
    "                w = sims[i1, i2]\n",
    "                if w >= threshold:\n",
    "                    edges.append({\n",
    "                        'source': f\"{y1}:{n1['topic']}\",\n",
    "                        'target': f\"{y2}:{n2['topic']}\",\n",
    "                        'kind': 'continue',\n",
    "                        'weight': w\n",
    "                    })\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f77eb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] Input DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6071 entries, 0 to 6070\n",
      "Data columns (total 56 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Unnamed: 0         6071 non-null   int64  \n",
      " 1   title              6071 non-null   object \n",
      " 2   ncode              6071 non-null   object \n",
      " 3   userid             6071 non-null   int64  \n",
      " 4   writer             6071 non-null   object \n",
      " 5   story              6071 non-null   object \n",
      " 6   biggenre           6071 non-null   int64  \n",
      " 7   genre              6071 non-null   int64  \n",
      " 8   gensaku            0 non-null      float64\n",
      " 9   keyword            6068 non-null   object \n",
      " 10  general_firstup    6071 non-null   object \n",
      " 11  general_lastup     6071 non-null   object \n",
      " 12  novel_type         6071 non-null   int64  \n",
      " 13  end                6071 non-null   int64  \n",
      " 14  general_all_no     6071 non-null   int64  \n",
      " 15  length             6071 non-null   int64  \n",
      " 16  time               6071 non-null   int64  \n",
      " 17  isstop             6071 non-null   int64  \n",
      " 18  isr15              6071 non-null   int64  \n",
      " 19  isbl               6071 non-null   int64  \n",
      " 20  isgl               6071 non-null   int64  \n",
      " 21  iszankoku          6071 non-null   int64  \n",
      " 22  istensei           6071 non-null   int64  \n",
      " 23  istenni            6071 non-null   int64  \n",
      " 24  pc_or_k            6071 non-null   int64  \n",
      " 25  global_point       6071 non-null   int64  \n",
      " 26  daily_point        6071 non-null   int64  \n",
      " 27  weekly_point       6071 non-null   int64  \n",
      " 28  monthly_point      6071 non-null   int64  \n",
      " 29  quarter_point      6071 non-null   int64  \n",
      " 30  yearly_point       6071 non-null   int64  \n",
      " 31  fav_novel_cnt      6071 non-null   int64  \n",
      " 32  impression_cnt     6071 non-null   int64  \n",
      " 33  review_cnt         6071 non-null   int64  \n",
      " 34  all_point          6071 non-null   int64  \n",
      " 35  all_hyoka_cnt      6071 non-null   int64  \n",
      " 36  sasie_cnt          6071 non-null   int64  \n",
      " 37  kaiwaritu          6071 non-null   int64  \n",
      " 38  novelupdated_at    6071 non-null   object \n",
      " 39  updated_at         6071 non-null   object \n",
      " 40  weekly_unique      6071 non-null   int64  \n",
      " 41  Group              6071 non-null   object \n",
      " 42  Genre              6071 non-null   object \n",
      " 43  title_tagged       6071 non-null   object \n",
      " 44  title_segment      6071 non-null   object \n",
      " 45  Segment            6071 non-null   object \n",
      " 46  Year               6071 non-null   int64  \n",
      " 47  title_character    6071 non-null   int64  \n",
      " 48  morethan100        6071 non-null   int64  \n",
      " 49  story_character    6071 non-null   int64  \n",
      " 50  term               6071 non-null   int64  \n",
      " 51  type               6071 non-null   int64  \n",
      " 52  morethan1w         6071 non-null   int64  \n",
      " 53  first_page         6071 non-null   object \n",
      " 54  first_update       6071 non-null   object \n",
      " 55  firstpage_segment  6071 non-null   object \n",
      "dtypes: float64(1), int64(38), object(17)\n",
      "memory usage: 2.6+ MB\n",
      "None\n",
      "[Debug] First few rows of the DataFrame:\n",
      "   Unnamed: 0                                              title    ncode  \\\n",
      "0           0  【ＷEＢ版】うちの弟子がいつのまにか人類最強になっていて、なんの才能もない師匠の俺が、それを...  N8759FG   \n",
      "1           1                                           水属性の魔法使い  N0022GD   \n",
      "2           2  【三章開始！】創成魔法の再現者　～『魔法が使えない』と実家を追放された天才少年、魔女の弟子と...  N0826GY   \n",
      "3           3  【アイテム無消費】だけが売りの俺。ダンジョン最下層へと追放される～だがそこで手に入れたアイテ...  N6395HB   \n",
      "4           4  【9月書籍・コミカライズ発売！】植物魔法チートでのんびり領主生活始めます～前世の知識を駆使し...  N0229FV   \n",
      "\n",
      "    userid   writer                                              story  \\\n",
      "0  1466155   アキライズン  \\n「貴方様が剣聖アリス様のお師匠様、大剣聖タクミ様で御座いますね」\\n「え、いや、人違いじ...   \n",
      "1  1866242     久宝　忠  【好きラノ2021年上期　新作部門第2位！】\\n\\n\\n書籍版\\n第二巻「水属性の魔法使い　...   \n",
      "2  1770987     みわもひ  「貴様は出来損ないだ、二度と我が家の敷居を跨ぐなぁ！」魔法が全ての国、とりわけ貴族だけが生ま...   \n",
      "3  1389003      まんじ  「くそっ！くそっ！くそくそくそ！」\\n\\n地面に跪き、怨嗟の声を上げる若者の名は――セドリ。...   \n",
      "4  1071283  りょうと　かえ  9月書籍第3巻、コミカライズ第2巻発売！\\nページ下部のバナーから公式サイトへと移動できます...   \n",
      "\n",
      "   biggenre  genre  gensaku  \\\n",
      "0         3    307      NaN   \n",
      "1         2    201      NaN   \n",
      "2         2    201      NaN   \n",
      "3         2    201      NaN   \n",
      "4         2    201      NaN   \n",
      "\n",
      "                                             keyword  ...  Year  \\\n",
      "0  勘違い ギャグ 人類最強 宇宙最強 ファンタジー ヒロイン多数 弟子 群像劇 男主人公 追放...  ...  2019   \n",
      "1  R15 異世界転生 シリアスではない ご都合主義 魔法 剣 男主人公 水属性魔法 王道ファン...  ...  2020   \n",
      "2  R15 残酷な描写あり 追放 ざまぁ 成長 成り上がり 主人公最強（予定） 魔法 幼馴染み ...  ...  2021   \n",
      "3  R15 残酷な描写あり オリジナル戦記 アイテム無限使用 復讐 ひれ伏せ もう遅い 無限召喚...  ...  2021   \n",
      "4  異世界転生 異世界 貴族 男主人公 魔法・スキル 開拓 主人公最強 チート 内政 ほのぼの ...  ...  2019   \n",
      "\n",
      "  title_character  morethan100  story_character  term  type  morethan1w  \\\n",
      "0              67            0              727    49    37           0   \n",
      "1               8            0              929     5     5           0   \n",
      "2              85            0              455    58    48           0   \n",
      "3              75            0              609    48    42           0   \n",
      "4              64            0              309    36    34           0   \n",
      "\n",
      "                                          first_page  \\\n",
      "0  「貴方様が剣聖アリス様のお師匠様、大剣聖タクミ様でございますね」\\n「は？ いや、人違いじゃ...   \n",
      "1  「涼さん、落ち着いて聞いてください」\\nそれは両親の死を告げる電話だった。\\n二年生になった...   \n",
      "2  新作！追放＆成長もののファンタジー。\\nまずは第一話、そしてよろしければ序章の五話まで読んで...   \n",
      "3  疾風怒濤。\\nそれは冒険者界隈を彗星の如く上り詰めて来たSランクパーティーの名だ。\\n今やこ...   \n",
      "4  それは突然起こった。\\n一瞬、頭痛が起こり――頭の中に膨大な知識が流れこんできたのだ。\\n手...   \n",
      "\n",
      "                             first_update  \\\n",
      "0  2019/01/27 20:15（改）2021/04/04 21:03 改稿   \n",
      "1  2020/04/01 00:31（改）2021/04/17 08:35 改稿   \n",
      "2  2021/04/29 19:09（改）2021/06/17 07:58 改稿   \n",
      "3  2021/07/06 15:37（改）2021/08/08 19:29 改稿   \n",
      "4  2019/10/20 18:03（改）2021/02/22 18:14 改稿   \n",
      "\n",
      "                                   firstpage_segment  \n",
      "0  「 貴方 様 が 剣聖 アリス 様 の お 師匠 様 、 大 剣聖 タクミ 様 で ござい ...  \n",
      "1  「 涼 さん 、 落ち着い て 聞い て ください 」 それ は 両親 の 死 を 告げる ...  \n",
      "2  新作 ！ 追放 ＆ 成長 もの の ファンタジー 。 まず は 第 一 話 、 そして よろ...  \n",
      "3  疾風 怒濤 。 それ は 冒険 者 界隈 を 彗星 の 如く 上り詰め て 来 た S ラン...  \n",
      "4  それ は 突然 起こっ た 。 一瞬 、 頭痛 が 起こり ―― 頭 の 中 に 膨大 な ...  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "[Debug] DataFrame after dropna:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6071 entries, 0 to 6070\n",
      "Data columns (total 56 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Unnamed: 0         6071 non-null   int64  \n",
      " 1   title              6071 non-null   object \n",
      " 2   ncode              6071 non-null   object \n",
      " 3   userid             6071 non-null   int64  \n",
      " 4   writer             6071 non-null   object \n",
      " 5   story              6071 non-null   object \n",
      " 6   biggenre           6071 non-null   int64  \n",
      " 7   genre              6071 non-null   int64  \n",
      " 8   gensaku            0 non-null      float64\n",
      " 9   keyword            6068 non-null   object \n",
      " 10  general_firstup    6071 non-null   object \n",
      " 11  general_lastup     6071 non-null   object \n",
      " 12  novel_type         6071 non-null   int64  \n",
      " 13  end                6071 non-null   int64  \n",
      " 14  general_all_no     6071 non-null   int64  \n",
      " 15  length             6071 non-null   int64  \n",
      " 16  time               6071 non-null   int64  \n",
      " 17  isstop             6071 non-null   int64  \n",
      " 18  isr15              6071 non-null   int64  \n",
      " 19  isbl               6071 non-null   int64  \n",
      " 20  isgl               6071 non-null   int64  \n",
      " 21  iszankoku          6071 non-null   int64  \n",
      " 22  istensei           6071 non-null   int64  \n",
      " 23  istenni            6071 non-null   int64  \n",
      " 24  pc_or_k            6071 non-null   int64  \n",
      " 25  global_point       6071 non-null   int64  \n",
      " 26  daily_point        6071 non-null   int64  \n",
      " 27  weekly_point       6071 non-null   int64  \n",
      " 28  monthly_point      6071 non-null   int64  \n",
      " 29  quarter_point      6071 non-null   int64  \n",
      " 30  yearly_point       6071 non-null   int64  \n",
      " 31  fav_novel_cnt      6071 non-null   int64  \n",
      " 32  impression_cnt     6071 non-null   int64  \n",
      " 33  review_cnt         6071 non-null   int64  \n",
      " 34  all_point          6071 non-null   int64  \n",
      " 35  all_hyoka_cnt      6071 non-null   int64  \n",
      " 36  sasie_cnt          6071 non-null   int64  \n",
      " 37  kaiwaritu          6071 non-null   int64  \n",
      " 38  novelupdated_at    6071 non-null   object \n",
      " 39  updated_at         6071 non-null   object \n",
      " 40  weekly_unique      6071 non-null   int64  \n",
      " 41  Group              6071 non-null   object \n",
      " 42  Genre              6071 non-null   object \n",
      " 43  title_tagged       6071 non-null   object \n",
      " 44  title_segment      6071 non-null   object \n",
      " 45  Segment            6071 non-null   object \n",
      " 46  Year               6071 non-null   int64  \n",
      " 47  title_character    6071 non-null   int64  \n",
      " 48  morethan100        6071 non-null   int64  \n",
      " 49  story_character    6071 non-null   int64  \n",
      " 50  term               6071 non-null   int64  \n",
      " 51  type               6071 non-null   int64  \n",
      " 52  morethan1w         6071 non-null   int64  \n",
      " 53  first_page         6071 non-null   object \n",
      " 54  first_update       6071 non-null   object \n",
      " 55  firstpage_segment  6071 non-null   object \n",
      "dtypes: float64(1), int64(38), object(17)\n",
      "memory usage: 2.6+ MB\n",
      "None\n",
      "[Debug] Unique years in the data: <IntegerArray>\n",
      "[2019, 2020, 2021, 2016, 2018, 2017, 2015, 2013, 2014, 2010, 2011, 2012, 2009,\n",
      " 2008]\n",
      "Length: 14, dtype: Int64\n",
      "[Debug] Year 2008: 3 documents\n",
      "[Debug] Year 2009: 17 documents\n",
      "[Debug] Year 2010: 56 documents\n",
      "[Debug] Year 2011: 125 documents\n",
      "[Debug] Year 2012: 242 documents\n",
      "[Debug] Year 2013: 335 documents\n",
      "[Debug] Year 2014: 392 documents\n",
      "[Debug] Year 2015: 502 documents\n",
      "[Debug] Year 2016: 556 documents\n",
      "[Debug] Year 2017: 566 documents\n",
      "[Debug] Year 2018: 680 documents\n",
      "[Debug] Year 2019: 798 documents\n",
      "[Debug] Year 2020: 1108 documents\n",
      "[Debug] Year 2021: 691 documents\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Main (Modified for Jupyter)\n",
    "# ------------------------\n",
    "\n",
    "# Define input and output paths directly\n",
    "input_file = 'Narou_global1w.csv'\n",
    "output_dir = './ten_out'\n",
    "min_docs_per_year = 10\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Debug: Check input DataFrame\n",
    "print('[Debug] Input DataFrame info:')\n",
    "print(df.info())\n",
    "print('[Debug] First few rows of the DataFrame:')\n",
    "print(df.head())\n",
    "\n",
    "df = df.dropna(subset=['story', 'Year'])\n",
    "print('[Debug] DataFrame after dropna:')\n",
    "print(df.info())\n",
    "\n",
    "# Debug: Check unique years\n",
    "df['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64')\n",
    "print('[Debug] Unique years in the data:', df['Year'].unique())\n",
    "\n",
    "# Debug: Check document count per year\n",
    "for y in sorted(df['Year'].unique()):\n",
    "    print(f'[Debug] Year {y}: {len(df[df[\"Year\"] == y])} documents')\n",
    "\n",
    "docs = df['story'].astype(str).tolist()\n",
    "years = df['Year'].astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decba2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 18:29:14,845 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 190/190 [00:02<00:00, 67.80it/s]\n",
      "2025-09-03 18:29:17,739 - BERTopic - Embedding - Completed ✓\n",
      "Batches: 100%|██████████| 190/190 [00:02<00:00, 67.80it/s]\n",
      "2025-09-03 18:29:17,739 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-03 18:29:17,740 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-03 18:29:17,740 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-03 18:29:29,346 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-03 18:29:29,346 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-03 18:29:29,346 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-03 18:29:29,346 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-03 18:29:29,797 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-03 18:29:29,797 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-09-03 18:29:29,797 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-03 18:29:29,797 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-09-03 18:29:39,295 - BERTopic - Representation - Completed ✓\n",
      "2025-09-03 18:29:39,295 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] Topics: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:30,  2.21s/it]\n",
      "14it [00:30,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# Fit BERTopic\n",
    "topic_model = build_model()\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# Debug: Check topics after fit_transform\n",
    "print('[Debug] Topics:', set(topics))\n",
    "\n",
    "# Topics over time\n",
    "topics_over_time = topic_model.topics_over_time(docs, years, nr_bins=len(set(years)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f8f7bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] Year 2009: 15 nodes\n",
      "[Debug] Year 2010: 26 nodes\n",
      "[Debug] Year 2011: 35 nodes\n",
      "[Debug] Year 2012: 38 nodes\n",
      "[Debug] Year 2013: 41 nodes\n",
      "[Debug] Year 2014: 40 nodes\n",
      "[Debug] Year 2015: 41 nodes\n",
      "[Debug] Year 2016: 41 nodes\n",
      "[Debug] Year 2017: 47 nodes\n",
      "[Debug] Year 2018: 47 nodes\n",
      "[Debug] Year 2019: 49 nodes\n",
      "[Debug] Year 2020: 44 nodes\n"
     ]
    }
   ],
   "source": [
    "# Build year->topic nodes\n",
    "year_topics = {}\n",
    "for y in sorted(set(years)):\n",
    "    nodes = []\n",
    "    df_y = topics_over_time[topics_over_time['Timestamp'].astype(int) == y]\n",
    "    for _, row in df_y.iterrows():\n",
    "        t = int(row['Topic'])\n",
    "        if t == -1:\n",
    "            continue\n",
    "        emb = topic_model.topic_embeddings_[t]\n",
    "        nodes.append({\n",
    "            'year': y,\n",
    "            'topic': t,\n",
    "            'top_terms': row['Words'],\n",
    "            'count': row['Frequency'],\n",
    "            'embedding': emb\n",
    "        })\n",
    "    if len(nodes) >= min_docs_per_year:\n",
    "        year_topics[y] = nodes\n",
    "\n",
    "# Debug: Check year_topics content\n",
    "for y, nodes in year_topics.items():\n",
    "    print(f'[Debug] Year {y}: {len(nodes)} nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e20e4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] All nodes: [{'id': '2009:0', 'year': 2009, 'topic': 0, 'top_terms': 'ゲーム-game ライフ-life, ゲーム-game 舞台, ゲーム-game キャラクター-character, ライン-line ゲーム-game, 魔法 冒険', 'count': 3}, {'id': '2009:1', 'year': 2009, 'topic': 1, 'top_terms': '！ アニメ-animation, 読む 呉れる, ？ ヒロイン-heroine, 中 作画, 勘弁 ！', 'count': 3}, {'id': '2009:2', 'year': 2009, 'topic': 2, 'top_terms': '彼女 下, 伝説 聖女, 為る 彼女, 来る 物語, 一人 少女', 'count': 3}, {'id': '2009:3', 'year': 2009, 'topic': 3, 'top_terms': '日本 人, 現代 日本, 有る 日本, 幽霊, 世界 日本', 'count': 2}, {'id': '2009:4', 'year': 2009, 'topic': 4, 'top_terms': '魔術 師, 魔法 使い, 魔術 才能, 最強 魔術, 魔法 使う', 'count': 2}]\n"
     ]
    }
   ],
   "source": [
    "# Build nodes.csv\n",
    "all_nodes = []\n",
    "for y, nodes in year_topics.items():\n",
    "    for n in nodes:\n",
    "        all_nodes.append({\n",
    "            'id': f\"{y}:{n['topic']}\",\n",
    "            'year': y,\n",
    "            'topic': n['topic'],\n",
    "            'top_terms': n['top_terms'],\n",
    "            'count': n['count']\n",
    "        })\n",
    "pd.DataFrame(all_nodes).to_csv(os.path.join(output_dir,'nodes.csv'), index=False)\n",
    "\n",
    "# Debug: Check all_nodes and edges before export\n",
    "print('[Debug] All nodes:', all_nodes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec93593f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done] Exported nodes.csv, edges.csv, sankey.csv, graph.graphml\n"
     ]
    }
   ],
   "source": [
    "# Build edges.csv\n",
    "edges = align_topics(year_topics, threshold=0.6)\n",
    "pd.DataFrame(edges).to_csv(os.path.join(output_dir,'edges.csv'), index=False)\n",
    "\n",
    "# Sankey\n",
    "sankey = [{'source':e['source'], 'target':e['target'], 'value':e['weight']} for e in edges]\n",
    "pd.DataFrame(sankey).to_csv(os.path.join(output_dir,'sankey.csv'), index=False)\n",
    "\n",
    "# GraphML\n",
    "with open(os.path.join(output_dir,'graph.graphml'),'w',encoding='utf-8') as f:\n",
    "    f.write(\"<?xml version='1.0' encoding='UTF-8'?>\\n\")\n",
    "    f.write(\"<graphml xmlns='http://graphml.graphdrawing.org/xmlns'>\\n\")\n",
    "    f.write(\"<graph edgedefault='directed'>\\n\")\n",
    "    for n in all_nodes:\n",
    "        f.write(f\"<node id='{n['id']}'><data key='label'>{n['top_terms']}</data></node>\\n\")\n",
    "    for e in edges:\n",
    "        f.write(f\"<edge source='{e['source']}' target='{e['target']}'><data key='weight'>{e['weight']}</data></edge>\\n\")\n",
    "    f.write(\"</graph></graphml>\")\n",
    "\n",
    "print(\"[Done] Exported nodes.csv, edges.csv, sankey.csv, graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d30b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
